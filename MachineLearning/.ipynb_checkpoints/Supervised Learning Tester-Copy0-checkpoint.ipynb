{
 "metadata": {
  "name": "",
  "signature": "sha256:ae4564d195268aca65c11eb1d563d9c4cf4614347da8c1e599f343471d5f8253"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline \n",
      "\n",
      "from IPython.display import Image\n",
      "import matplotlib as mlp\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import os\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "\n",
      "from sklearn import cross_validation\n",
      "from sklearn import tree\n",
      "from sklearn import svm\n",
      "from sklearn import ensemble\n",
      "from sklearn import neighbors\n",
      "from sklearn import linear_model\n",
      "from sklearn import metrics\n",
      "from sklearn import preprocessing\n",
      "\n",
      "plt.style.use('fivethirtyeight') # Good looking plots\n",
      "pd.set_option('display.max_columns', None) # Display any number of columns\n",
      "\n",
      "import re\n",
      "_DATA_DIR = r'../../_python_data\\Biological Response'\n",
      "_DATA_DIR = re.sub(r\"\\\\\", \"/\", _DATA_DIR)\n",
      "_CHURN_DATA_PATH = _DATA_DIR + '/train.csv'\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "def llfun(act, pred):\n",
      "    epsilon = 1e-15\n",
      "    pred = sp.maximum(epsilon, pred)\n",
      "    pred = sp.minimum(1-epsilon, pred)\n",
      "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
      "    ll = ll * -1.0/len(act)\n",
      "    return ll\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(_CHURN_DATA_PATH)\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = df['Activity'].as_matrix().astype(np.int)\n",
      "\n",
      "# Drop the redundant columns from dataframe\n",
      "df.drop(['Activity'], axis=1, inplace=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('There are {} instances for churn class and {} instances for not-churn classes.'.format(y.sum(), y.shape[0] - y.sum()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Ratio of churn class over all instances: {:.2f}'.format(float(y.sum()) / y.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kind of unbalanced data, do not you think? I will try to handle this unbalance in the cross validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After preprocessing, dataframe is ready to be represented as matrix that is amenable to Scikit Learn. I already separated the labels, so I would just convert the dataframe into a numpy matrix. Why I did not handle True and False some may ask. It turns out that booleans False and True are actually subclasses of integers in Python. If you try to do False + True in a Python REPL, you would get 1. That is because True is represented as 1 and False is represented as 0. Numpy uses this boolean information to convert the booleans into matrix. I just need to coerce astype(np.float) when I convert the pandas dataframe to numpy matrix, which I will do exactly as next step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scaler = preprocessing.StandardScaler()\n",
      "X = scaler.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stratified_cv(X, y, clf_class, shuffle=True, n_folds=10, **kwargs):\n",
      "    stratified_k_fold = cross_validation.StratifiedKFold(y, n_folds=n_folds, shuffle=shuffle)\n",
      "    y_pred = y.copy()\n",
      "    for ii, jj in stratified_k_fold:\n",
      "        X_train, X_test = X[ii], X[jj]\n",
      "        y_train = y[ii]\n",
      "        clf = clf_class(**kwargs)\n",
      "        clf.fit(X_train,y_train)\n",
      "        y_pred[jj] = clf.predict(X_test)\n",
      "    return y_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, linear_model.PassiveAggressiveClassifier)\n",
      "print('Passive Aggressive Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Passive Aggressive Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "pass_agg_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, ensemble.GradientBoostingClassifier)\n",
      "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Gradient Boosting Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "grad_ens_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, svm.SVC)\n",
      "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Support vector machine(SVM) LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "svm_svc_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, ensemble.RandomForestClassifier)\n",
      "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Random Forest Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "random_forest_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, neighbors.KNeighborsClassifier)\n",
      "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "k_neighbors_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, linear_model.LogisticRegression)\n",
      "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Logistic Regression LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "logistic_reg_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = [0 for ii in y.tolist()]\n",
      "print('Dump Classifier:           {:.2f}'.format(metrics.accuracy_score(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred))); # ignore the warning as they are all 0\n",
      "dumb_conf_matrix = metrics.confusion_matrix(y, y_pred); # ignore the warning as they are all 0\n",
      "##\n",
      "decision_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, tree.DecisionTreeClassifier))\n",
      "ridge_clf_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.RidgeClassifier))\n",
      "logistic_reg_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.LogisticRegression))\n",
      "##\n",
      "conf_matrix = {\n",
      "                1: {\n",
      "                    'matrix': pass_agg_conf_matrix,\n",
      "                    'title': 'Passive Aggressive',\n",
      "                   },\n",
      "                2: {\n",
      "                    'matrix': grad_ens_conf_matrix,\n",
      "                    'title': 'Gradient Boosting',\n",
      "                   },\n",
      "                3: {\n",
      "                    'matrix': decision_conf_matrix,\n",
      "                    'title': 'Decision Tree',\n",
      "                   },\n",
      "                4: {\n",
      "                    'matrix': ridge_clf_conf_matrix,\n",
      "                    'title': 'Ridge',\n",
      "                   },\n",
      "                5: {\n",
      "                    'matrix': svm_svc_conf_matrix,\n",
      "                    'title': 'Support Vector Machine',\n",
      "                   },\n",
      "                6: {\n",
      "                    'matrix': random_forest_conf_matrix,\n",
      "                    'title': 'Random Forest',\n",
      "                   },\n",
      "                7: {\n",
      "                    'matrix': k_neighbors_conf_matrix,\n",
      "                    'title': 'K Nearest Neighbors',\n",
      "                   },\n",
      "                8: {\n",
      "                    'matrix': logistic_reg_conf_matrix,\n",
      "                    'title': 'Logistic Regression',\n",
      "                   },\n",
      "                9: {\n",
      "                    'matrix': dumb_conf_matrix,\n",
      "                    'title': 'Dumb',\n",
      "                   },\n",
      "}\n",
      "fix, ax = plt.subplots(figsize=(16, 12))\n",
      "plt.suptitle('Confusion Matrix of Various Classifiers')\n",
      "for ii, values in conf_matrix.items():\n",
      "    matrix = values['matrix']\n",
      "    title = values['title']\n",
      "    plt.subplot(3, 3, ii) # starts from 1\n",
      "    plt.title(title);\n",
      "    sns.heatmap(matrix, annot=True,  fmt='');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scaled and Polynomial Features\n",
      "If you think, when some of the features come together, they could form a much more powerful feature, or just by getting the square of the feature would be powerful feature, then Scikit-Learn has something that quite fits to your needs. Let's aasume I have [x, y] feature vector and I am interested in [1, x, y, x^2, xy, y^2], in the preprocessing step, I could use PolynomialFeatures of Scikit-Learn to build that feature matrix. If I just want to only get the interaction features(not x^2, then it is enough to pass interaction_only=True and include_bias=False. If you want to get higher order Polynomial features(say nth degree), pass degree=n optional parameter to Polynomial Features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)\n",
      "polynomial_features = preprocessing.PolynomialFeatures()\n",
      "X = polynomial_features.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, linear_model.PassiveAggressiveClassifier)\n",
      "print('Passive Aggressive Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Passive Aggressive Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "pass_agg_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, ensemble.GradientBoostingClassifier)\n",
      "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Gradient Boosting Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "grad_ens_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, svm.SVC)\n",
      "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Support vector machine(SVM) LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "svm_svc_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, ensemble.RandomForestClassifier)\n",
      "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Random Forest Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "random_forest_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, neighbors.KNeighborsClassifier)\n",
      "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "k_neighbors_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, linear_model.LogisticRegression)\n",
      "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Logistic Regression LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "logistic_reg_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = [0 for ii in y.tolist()]\n",
      "print('Dump Classifier:           {:.2f}'.format(metrics.accuracy_score(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred))); # ignore the warning as they are all 0\n",
      "dumb_conf_matrix = metrics.confusion_matrix(y, y_pred); # ignore the warning as they are all 0\n",
      "##\n",
      "decision_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, tree.DecisionTreeClassifier))\n",
      "ridge_clf_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.RidgeClassifier))\n",
      "logistic_reg_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.LogisticRegression))\n",
      "##\n",
      "conf_matrix = {\n",
      "                1: {\n",
      "                    'matrix': pass_agg_conf_matrix,\n",
      "                    'title': 'Passive Aggressive',\n",
      "                   },\n",
      "                2: {\n",
      "                    'matrix': grad_ens_conf_matrix,\n",
      "                    'title': 'Gradient Boosting',\n",
      "                   },\n",
      "                3: {\n",
      "                    'matrix': decision_conf_matrix,\n",
      "                    'title': 'Decision Tree',\n",
      "                   },\n",
      "                4: {\n",
      "                    'matrix': ridge_clf_conf_matrix,\n",
      "                    'title': 'Ridge',\n",
      "                   },\n",
      "                5: {\n",
      "                    'matrix': svm_svc_conf_matrix,\n",
      "                    'title': 'Support Vector Machine',\n",
      "                   },\n",
      "                6: {\n",
      "                    'matrix': random_forest_conf_matrix,\n",
      "                    'title': 'Random Forest',\n",
      "                   },\n",
      "                7: {\n",
      "                    'matrix': k_neighbors_conf_matrix,\n",
      "                    'title': 'K Nearest Neighbors',\n",
      "                   },\n",
      "                8: {\n",
      "                    'matrix': logistic_reg_conf_matrix,\n",
      "                    'title': 'Logistic Regression',\n",
      "                   },\n",
      "                9: {\n",
      "                    'matrix': dumb_conf_matrix,\n",
      "                    'title': 'Dumb',\n",
      "                   },\n",
      "}\n",
      "fix, ax = plt.subplots(figsize=(16, 12))\n",
      "plt.suptitle('Confusion Matrix of Various Classifiers')\n",
      "for ii, values in conf_matrix.items():\n",
      "    matrix = values['matrix']\n",
      "    title = values['title']\n",
      "    plt.subplot(3, 3, ii) # starts from 1\n",
      "    plt.title(title);\n",
      "    sns.heatmap(matrix, annot=True,  fmt='');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)\n",
      "scaler = preprocessing.StandardScaler()\n",
      "X = scaler.fit_transform(X)\n",
      "polynomial_features = preprocessing.PolynomialFeatures()\n",
      "X = polynomial_features.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, linear_model.PassiveAggressiveClassifier)\n",
      "print('Passive Aggressive Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Passive Aggressive Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "pass_agg_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, ensemble.GradientBoostingClassifier)\n",
      "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Gradient Boosting Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "grad_ens_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, svm.SVC)\n",
      "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Support vector machine(SVM) LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "svm_svc_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, ensemble.RandomForestClassifier)\n",
      "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Random Forest Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "random_forest_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, neighbors.KNeighborsClassifier)\n",
      "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "k_neighbors_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = stratified_cv(X, y, linear_model.LogisticRegression)\n",
      "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Logistic Regression LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "logistic_reg_conf_matrix = metrics.confusion_matrix(y, y_pred)\n",
      "y_pred = [0 for ii in y.tolist()]\n",
      "print('Dump Classifier:           {:.2f}'.format(metrics.accuracy_score(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred))); # ignore the warning as they are all 0\n",
      "dumb_conf_matrix = metrics.confusion_matrix(y, y_pred); # ignore the warning as they are all 0\n",
      "##\n",
      "decision_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, tree.DecisionTreeClassifier))\n",
      "ridge_clf_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.RidgeClassifier))\n",
      "logistic_reg_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.LogisticRegression))\n",
      "##\n",
      "conf_matrix = {\n",
      "                1: {\n",
      "                    'matrix': pass_agg_conf_matrix,\n",
      "                    'title': 'Passive Aggressive',\n",
      "                   },\n",
      "                2: {\n",
      "                    'matrix': grad_ens_conf_matrix,\n",
      "                    'title': 'Gradient Boosting',\n",
      "                   },\n",
      "                3: {\n",
      "                    'matrix': decision_conf_matrix,\n",
      "                    'title': 'Decision Tree',\n",
      "                   },\n",
      "                4: {\n",
      "                    'matrix': ridge_clf_conf_matrix,\n",
      "                    'title': 'Ridge',\n",
      "                   },\n",
      "                5: {\n",
      "                    'matrix': svm_svc_conf_matrix,\n",
      "                    'title': 'Support Vector Machine',\n",
      "                   },\n",
      "                6: {\n",
      "                    'matrix': random_forest_conf_matrix,\n",
      "                    'title': 'Random Forest',\n",
      "                   },\n",
      "                7: {\n",
      "                    'matrix': k_neighbors_conf_matrix,\n",
      "                    'title': 'K Nearest Neighbors',\n",
      "                   },\n",
      "                8: {\n",
      "                    'matrix': logistic_reg_conf_matrix,\n",
      "                    'title': 'Logistic Regression',\n",
      "                   },\n",
      "                9: {\n",
      "                    'matrix': dumb_conf_matrix,\n",
      "                    'title': 'Dumb',\n",
      "                   },\n",
      "}\n",
      "fix, ax = plt.subplots(figsize=(16, 12))\n",
      "plt.suptitle('Confusion Matrix of Various Classifiers')\n",
      "for ii, values in conf_matrix.items():\n",
      "    matrix = values['matrix']\n",
      "    title = values['title']\n",
      "    plt.subplot(3, 3, ii) # starts from 1\n",
      "    plt.title(title);\n",
      "    sns.heatmap(matrix, annot=True,  fmt='');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}