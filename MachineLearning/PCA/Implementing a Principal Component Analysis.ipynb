{
 "metadata": {
  "name": "",
  "signature": "sha256:9aab75627394a29435db05a78f5e29b8e5c2d8b94d5a5c456a98139cadc08a68"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this article I want to explain how a Principal Component Analysis (PCA) works by implementing it in Python step by step. At the end we will compare the results to the more convenient Python PCA()classes that are available through the popular matplotlib and scipy libraries and discuss how they differ.\n",
      "\n",
      "\n",
      "\n",
      "Sections\n",
      "Introduction\n",
      "Generating 3-dimensional sample data\n",
      "The step by step approach\n",
      "1. Taking the whole dataset ignoring the class labels\n",
      "2.  Compute the d-dimensional mean vector\n",
      "3. Computing the scatter matrix (alternatively, the covariance matrix)\n",
      "4. Computing eigenvectors and corresponding eigenvalues\n",
      "5. Ranking and choosing k eigenvectors\n",
      "6. Transforming the samples onto the new subspace\n",
      "Using the PCA() class from the matplotlib.mlab library\n",
      " Differences between the step by step approach and matplotlib.mlab.PCA()\n",
      "Using the PCA() class from the sklearn.decomposition library to confirm our results\n",
      "Appendix A: The effect of scaling and mean centering of variables prior to a Principal Component Analysis\n",
      "Appendix B: PCA based on the covariance matrix vs. correlation matrix\n",
      "\n",
      "Introduction\n",
      "[back to top]\n",
      "The main purposes of a principal component analysis are the analysis of data to identify patterns and finding patterns to reduce the dimensions of the dataset with minimal loss of information.\n",
      "\n",
      "Here, our desired outcome of the principal component analysis is to project a feature space (our dataset consisting of n x d-dimensional samples) onto a smaller subspace that represents our data \"well\". A possible application would be a pattern classification task, where we want to reduce the computational costs and the error of parameter estimation by reducing the number of dimensions of our feature space by extracting a subspace that describes our data \"best\".\n",
      "\n",
      "About the notation:\n",
      "In the following sections, we will use a bold-face and lower-case letters for denoting column vectors (e.g., e) and bold-face upper-case letters for matrices (e.g., W)\n",
      "\n",
      "Principal Component Analysis (PCA) Vs. Multiple Discriminant Analysis (MDA)\n",
      "[back to top]\n",
      "Both Multiple Discriminant Analysis (MDA) and Principal Component Analysis (PCA) are linear transformation methods and closely related to each other. In PCA, we are interested to find the directions (components) that maximize the variance in our dataset, where in MDA, we are additionally interested to find the directions that maximize the separation (or discrimination) between different classes (for example, in pattern classification problems where our dataset consists of multiple classes. In contrast two PCA, which ignores the class labels).\n",
      "In other words, via PCA, we are projecting the entire set of data (without class labels) onto a different subspace, and in MDA, we are trying to determine a suitable subspace to distinguish between patterns that belong to different classes. Or, roughly speaking in PCA we are trying to find the axes with maximum variances where the data is most spread (within a class, since PCA treats the whole data set as one class), and in MDA we are additionally maximizing the spread between classes. \n",
      "In typical pattern recognition problems, a PCA is often followed by an MDA.\n",
      "\n",
      "What is a \"good\" subspace?\n",
      "[back to top]\n",
      "Let's assume that our goal is to reduce the dimensions of a d-dimensional dataset by projecting it onto a k-dimensional subspace (where k < d). So, how do we know what size we should choose for k, and how do we know if we have a feature space that represents our data \"well\"?\n",
      "Later, we will compute eigenvectors (the components) from our data set and collect them in a so-called scatter-matrix (or alternatively calculate them from the covariance matrix). Each of those eigenvectors is associated with an eigenvalue, which tell us about the \"length\" or \"magnitude\" of the eigenvectors. If we observe that all the eigenvalues are of very similar magnitude, this is a good indicator that our data is already in a \"good\" subspace. Or if some of the eigenvalues are much much higher than others, we might be interested in keeping only those eigenvectors with the much larger eigenvalues, since they contain more information about our data distribution. Vice versa, eigenvalues that are close to 0 are less informative and we might consider in dropping those when we construct the new feature subspace.\n",
      "\n",
      "Summarizing the PCA approach\n",
      "[back to top]\n",
      "Listed below are the 6 general steps for performing a principal component analysis, which we will investigate in the following sections.\n",
      "\n",
      "Take the whole dataset consisting of d-dimensional samples ignoring the class labels\n",
      "Compute the d-dimensional mean vector (i.e., the means for every dimension of the whole dataset)\n",
      "Compute the scatter matrix (alternatively, the covariance matrix) of the whole data set\n",
      "Compute eigenvectors and corresponding eigenvalues\n",
      "Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d x k dimensional matrix W (where every column represents an eigenvector)\n",
      "Use this d x k eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the mathematical equation:\n",
      "\n",
      "(where x is a d x 1 -dimensional vector representing one sample, and y is the transformed k x 1 -dimensional sample in the new subspace.)\n",
      "\n",
      "Generating some 3-dimensional sample data\n",
      "[back to top]\n",
      "For the following example, we will generate 40x3-dimensional samples randomly drawn from a multivariate Gaussian distribution. \n",
      "Here, we will assume that the samples stem from two different classes, where one half (i.e., 20) samples of our data set are labeled \u03c91 (class 1) and the other half \u03c92 (class 2).\n",
      "\n",
      "\n",
      "Why are we chosing a 3-dimensional sample?\n",
      "[back to top]\n",
      "The problem of multi-dimensional data is its visualization, which would make it quite tough to follow our example principal component analysis (at least visually). We could also choose a 2-dimensional sample data set for the following examples, but since the goal of the PCA in an \"Diminsionality Reduction\" application is to drop at least one of the dimensions, I find it more intuitive and visually appealing to start with a 3-dimensional dataset that we reduce to an 2-dimensional dataset by dropping 1 dimension."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "np.random.seed(1) # random seed for consistency\n",
      "\n",
      "# A reader pointed out that Python 2.7 would raise a\n",
      "# \"ValueError: object of too small depth for desired array\".\n",
      "# This can be avoided by choosing a smaller random seed, e.g. 1\n",
      "# or by completely omitting this line, since I just used the random seed for\n",
      "# consistency.\n",
      "\n",
      "mu_vec1 = np.array([0,0,0])\n",
      "cov_mat1 = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
      "class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, 20).T\n",
      "assert class1_sample.shape == (3,20), \"The matrix has not the dimensions 3x20\"\n",
      "\n",
      "mu_vec2 = np.array([1,1,1])\n",
      "cov_mat2 = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
      "class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, 20).T\n",
      "assert class1_sample.shape == (3,20), \"The matrix has not the dimensions 3x20\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the code above, we created two 3x20-datasets - one dataset for each class \u03c91 and \u03c92 -\n",
      "where each column can be pictured as a 3-dimensional vector\n",
      "\n",
      "so that our dataset will have the form \n",
      "\n",
      "\n",
      "Just to get a rough idea how the samples of our two classes \u03c91 and \u03c92 are distributed, let us plot them in a 3D scatter plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from mpl_toolkits.mplot3d import proj3d\n",
      "\n",
      "fig = plt.figure(figsize=(8,8))\n",
      "ax = fig.add_subplot(111, projection='3d')\n",
      "plt.rcParams['legend.fontsize'] = 10\n",
      "ax.plot(class1_sample[0,:], class1_sample[1,:],\\\n",
      "    class1_sample[2,:], 'o', markersize=8, color='blue', alpha=0.5, label='class1')\n",
      "ax.plot(class2_sample[0,:], class2_sample[1,:],\\\n",
      "    class2_sample[2,:], '^', markersize=8, alpha=0.5, color='red', label='class2')\n",
      "\n",
      "plt.title('Samples for class 1 and class 2')\n",
      "ax.legend(loc='upper right')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Taking the whole dataset ignoring the class labels\n",
      "[back to top]\n",
      "Because we don't need class labels for the PCA analysis, let us merge the samples for our 2 classes into one 3x40-dimensional array."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_samples = np.concatenate((class1_sample, class2_sample), axis=1)\n",
      "assert all_samples.shape == (3,40), \"The matrix has not the dimensions 3x40\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2. Computing the d-dimensional mean vector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_x = np.mean(all_samples[0,:])\n",
      "mean_y = np.mean(all_samples[1,:])\n",
      "mean_z = np.mean(all_samples[2,:])\n",
      "\n",
      "mean_vector = np.array([[mean_x],[mean_y],[mean_z]])\n",
      "\n",
      "print('Mean Vector:\\n', mean_vector)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Mean Vector:\\n', array([[ 0.41667492],\n",
        "       [ 0.69848315],\n",
        "       [ 0.49242335]]))\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}