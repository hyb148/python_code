{
 "metadata": {
  "name": "",
  "signature": "sha256:391ce0ae70e14207eadd41dfc66fef2df8f97d80aa136a5456c3cbb10c13e04e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Introduction to Machine Learning with Python and Scikit-Learn\n",
      "http://kukuruku.co/hub/python/introduction-to-machine-learning-with-python-andscikit-learn"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Loading\n",
      "First of all, the data should be loaded into memory, so that we could work with it. \n",
      "The Scikit-Learn library uses NumPy arrays in its implementation, so we will use NumPy to load *.csv files. \n",
      "Let\u2019s download one of the datasets from the UCI Machine Learning Repository.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import urllib\n",
      "# url with dataset\n",
      "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
      "# download the file\n",
      "raw_data = urllib.urlopen(url)\n",
      "# load the CSV file as a numpy matrix\n",
      "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
      "# separate the data from the target attributes\n",
      "X = dataset[:,0:7]\n",
      "y = dataset[:,8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Data Normalization\n",
      "\n",
      "All of us know well that the majority of gradient methods (on which almost all machine learning algorithms are based) are highly sensitive to data scaling. Therefore, before running an algorithm, we should perform either normalization, or the so-called standardization. Normalization involves replacing nominal features, so that each of them would be in the range from 0 to 1. As for standardization, it involves data pre-processing, after which each feature has an average 0 and 1 dispersion. The Scikit-Learn library provides ready-made functions for this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "# normalize the data attributes\n",
      "normalized_X = preprocessing.normalize(X)\n",
      "# standardize the data attributes\n",
      "standardized_X = preprocessing.scale(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Feature Selection\n",
      "\n",
      "It\u2019s no secret that the most important thing in solving a task is the ability to properly choose or even create features. It\u2019s called Feature Selection and Feature Engineering. While Future Engineering is quite a creative process and relies more on intuition and expert knowledge, there are plenty of ready-made algorithms for Feature Selection. Tree algorithms allow to compute the informativeness of features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "model = ExtraTreesClassifier()\n",
      "model.fit(X, y)\n",
      "# display the relative importance of each attribute\n",
      "print(model.feature_importances_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All other methods are based on the effective search of subsets of features in order to find the best subset, on which the developed model gives the best quality. One of these search algorithms is the Recursive Feature Elimination Algorithm that is also available in the Scikit-Learn library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "# create the RFE model and select 3 attributes\n",
      "rfe = RFE(model, 3)\n",
      "rfe = rfe.fit(X, y)\n",
      "# summarize the selection of the attributes\n",
      "print(rfe.support_)\n",
      "print(rfe.ranking_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Algorithm Development\n",
      "\n",
      "As I have said, Scikit-Learn has implemented all the basic algorithms of machine learning. Let\u2019s take a look at some of them.\n",
      "\n",
      "Logistic Regression\n",
      "Most often used for solving tasks of classification (binary), but multiclass classification (the so-called one-vs-all method) is also allowed. The advantage of this algorithm is that there\u2019s the probability of belonging to a class for each object at the output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(X, y)\n",
      "print(model)\n",
      "# make predictions\n",
      "expected = y\n",
      "predicted = model.predict(X)\n",
      "# summarize the fit of the model\n",
      "print(metrics.classification_report(expected, predicted))\n",
      "print(metrics.confusion_matrix(expected, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naive Bayes\n",
      "Is also one of the most well-known machine learning algorithms, the main task of which is to restore the density of data distribution of the training sample. This method often provides good quality in multiclass classification problems."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "model = GaussianNB()\n",
      "model.fit(X, y)\n",
      "print(model)\n",
      "# make predictions\n",
      "expected = y\n",
      "predicted = model.predict(X)\n",
      "# summarize the fit of the model\n",
      "print(metrics.classification_report(expected, predicted))\n",
      "print(metrics.confusion_matrix(expected, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "k-Nearest Neighbours\n",
      "The kNN (k-Nearest Neighbors) method is often used as part of a more complex classification algorithm. For instance, we can use its estimate as an object\u2019s feature. Sometimes, a simple kNN provides great quality on well-chosen features. When parameters (metrics mostly) are set well, the algorithm often gives good quality in regression problems."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "# fit a k-nearest neighbor model to the data\n",
      "model = KNeighborsClassifier()\n",
      "model.fit(X, y)\n",
      "print(model)\n",
      "# make predictions\n",
      "expected = y\n",
      "predicted = model.predict(X)\n",
      "# summarize the fit of the model\n",
      "print(metrics.classification_report(expected, predicted))\n",
      "print(metrics.confusion_matrix(expected, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Decision Trees\n",
      "Classification and Regression Trees (CART) are often used in problems, in which objects have category features and used for regression and classification problems. The trees are very well suited for multiclass classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "# fit a CART model to the data\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(X, y)\n",
      "print(model)\n",
      "# make predictions\n",
      "expected = y\n",
      "predicted = model.predict(X)\n",
      "# summarize the fit of the model\n",
      "print(metrics.classification_report(expected, predicted))\n",
      "print(metrics.confusion_matrix(expected, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support Vector Machines\n",
      "SVM (Support Vector Machines) is one of the most popular machine learning algorithms used mainly for the classification problem. As well as logistic regression, SVM allows multi-class classification with the help of the one-vs-all method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.svm import SVC\n",
      "# fit a SVM model to the data\n",
      "model = SVC()\n",
      "model.fit(X, y)\n",
      "print(model)\n",
      "# make predictions\n",
      "expected = y\n",
      "predicted = model.predict(X)\n",
      "# summarize the fit of the model\n",
      "print(metrics.classification_report(expected, predicted))\n",
      "print(metrics.confusion_matrix(expected, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How to Optimize Algorithm Parameters\n",
      "One of the most difficult stages in creating really efficient algorithms is choosing correct parameters. It\u2019s usually easier with experience, but one way or another, we have to do the search. Fortunately, Scikit-Learn provides many implemented functions for this purpose.\n",
      "\n",
      "As an example, let\u2019s take a look at the selection of the regularization parameter, in which several values are searched in turn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "# prepare a range of alpha values to test\n",
      "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
      "# create and fit a ridge regression model, testing each alpha\n",
      "model = Ridge()\n",
      "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
      "grid.fit(X, y)\n",
      "print(grid)\n",
      "# summarize the results of the grid search\n",
      "print(grid.best_score_)\n",
      "print(grid.best_estimator_.alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes it is more efficient to randomly select a parameter from the given range, estimate the algorithm quality for this parameter and choose the best one."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.stats import uniform as sp_rand\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.grid_search import RandomizedSearchCV\n",
      "# prepare a uniform distribution to sample for the alpha parameter\n",
      "param_grid = {'alpha': sp_rand()}\n",
      "# create and fit a ridge regression model, testing random alpha values\n",
      "model = Ridge()\n",
      "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
      "rsearch.fit(X, y)\n",
      "print(rsearch)\n",
      "# summarize the results of the random parameter search\n",
      "print(rsearch.best_score_)\n",
      "print(rsearch.best_estimator_.alpha)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}