{
 "metadata": {
  "name": "",
  "signature": "sha256:c21daa5514c1e1493b6c18a41893bd4f2dccf930ea18b95d09bffccc68b744ec"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline \n",
      "\n",
      "from IPython.display import Image\n",
      "import matplotlib as mlp\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import os\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "\n",
      "from sklearn import cross_validation\n",
      "from sklearn import tree\n",
      "from sklearn import svm\n",
      "from sklearn import ensemble\n",
      "from sklearn import neighbors\n",
      "from sklearn import linear_model\n",
      "from sklearn import metrics\n",
      "from sklearn import preprocessing\n",
      "\n",
      "from pandas import read_pickle, DataFrame\n",
      "\n",
      "plt.style.use('fivethirtyeight') # Good looking plots\n",
      "pd.set_option('display.max_columns', None) # Display any number of columns\n",
      "\n",
      "import re\n",
      "_DATA_DIR = r'../../_python_data\\Biological Response'\n",
      "_DATA_DIR = re.sub(r\"\\\\\", \"/\", _DATA_DIR)\n",
      "_CHURN_DATA_PATH = _DATA_DIR + '/train.csv'\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "import scipy as sp\n",
      "\n",
      "def llfun(act, pred):\n",
      "    epsilon = 1e-15\n",
      "    pred = sp.maximum(epsilon, pred)\n",
      "    pred = sp.minimum(1-epsilon, pred)\n",
      "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
      "    ll = ll * -1.0/len(act)\n",
      "    return ll\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df.to_pickle('dataframe.pkl')\n",
      "df = pd.read_pickle('dataframe.pkl')\n",
      "#df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df.fillna(0)\n",
      "print df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "      x     y    dx    dy       hdg        spd       yaw  dx2  dy2       acc  \\\n",
        "0   0.0   0.0   0.0   0.0  0.000000   0.000000  0.000000  0.0  0.0  0.000000   \n",
        "1  18.6 -11.1  18.6 -11.1 -0.538044  21.660332 -0.538044  0.0  0.0  0.000000   \n",
        "2  36.1 -21.9  17.5 -10.8 -0.545294  20.564289 -0.552929 -1.1  0.3  1.140175   \n",
        "3  53.7 -32.6  17.6 -10.7 -0.545606  20.597330 -0.546248  0.1  0.1  0.141421   \n",
        "4  70.1 -42.8  16.4 -10.2 -0.548145  19.313208 -0.556404 -1.2  0.5  1.300000   \n",
        "\n",
        "       dyaw       dspd     dyaw2  yaw_spd_ratio  acc_spd_ratio  \\\n",
        "0  0.000000   0.000000  0.000000       0.000000       0.000000   \n",
        "1  0.000000  21.660332 -0.538044      -0.024840       0.000000   \n",
        "2  2.875341  -1.096043 -0.014885      -0.026888       0.055444   \n",
        "3  0.785398   0.033040  0.006681      -0.026520       0.006866   \n",
        "4  2.746802  -1.284122 -0.010156      -0.028810       0.067311   \n",
        "\n",
        "   dyaw_spd_ratio  dyaw_acc_ratio  dspd_spd_ratio  \n",
        "0        0.000000        0.000000        0.000000  \n",
        "1        0.000000        0.000000        1.000000  \n",
        "2        0.139822        2.521841       -0.053298  \n",
        "3        0.038131        5.553604        0.001604  \n",
        "4        0.142224        2.112924       -0.066489  \n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "print('There are {} instances for churn class and {} instances for not-churn classes.'.format(y.sum(), y.shape[0] - y.sum()))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "print('Ratio of churn class over all instances: {:.2f}'.format(float(y.sum()) / y.shape[0]))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kind of unbalanced data, do not you think? I will try to handle this unbalance in the cross validation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "df.head()"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After preprocessing, dataframe is ready to be represented as matrix that is amenable to Scikit Learn. I already separated the labels, so I would just convert the dataframe into a numpy matrix. Why I did not handle True and False some may ask. It turns out that booleans False and True are actually subclasses of integers in Python. If you try to do False + True in a Python REPL, you would get 1. That is because True is represented as 1 and False is represented as 0. Numpy uses this boolean information to convert the booleans into matrix. I just need to coerce astype(np.float) when I convert the pandas dataframe to numpy matrix, which I will do exactly as next step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.shape\n",
      "len(X)\n",
      "X = np.nan_to_num(X)\n",
      "print np.sum(np.isnan(X))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I have 3333 instances and has 18 dimension feature vectors for each instances.\n",
      "\n",
      "Since the features have quite different value ranges and some of them are discrete and some of them take continuous values, I need to scale them first. Removing mean and dividing the standard deviation of features respectively. Generally, this is one of the most commonly used preprocessing step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scaler = preprocessing.StandardScaler()\n",
      "X = scaler.fit_transform(X)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After preprocessed X, I could be sure that all of the features are in in more or less in the same range. However, one may think twice if the features are not nearly uniformed distribution(for example if a variable is categorical), you may do a different preprocessing as preprocessing makes the categorical variables like continuous variables.\n",
      "\n",
      "Cross Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stratified_cv(X, y, clf_class, shuffle=True, n_folds=10, **kwargs):\n",
      "    stratified_k_fold = cross_validation.StratifiedKFold(y, n_folds=n_folds, shuffle=shuffle)\n",
      "    y_pred = y.copy()\n",
      "    for ii, jj in stratified_k_fold:\n",
      "        X_train, X_test = X[ii], X[jj]\n",
      "        y_train = y[ii]\n",
      "        clf = clf_class(**kwargs)\n",
      "        clf.fit(X_train,y_train)\n",
      "        y_pred[jj] = clf.predict(X_test)\n",
      "    return y_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I am using Stratified K Fold because there the classes are unbalanced. I do not want any folds to have only 1 particular class or even 1 class dominating the other one as it may create a bias in that particular fold. Stratification makes sure that the percentage of samples for each class is similar across folds(if not same). If you do not have an unbalanced problem, KFold would work fine."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = np.ones(len(X))\n",
      "print shape(y), sum(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, linear_model.PassiveAggressiveClassifier)\n",
      "print('Passive Aggressive Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Passive Aggressive Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, ensemble.GradientBoostingClassifier)\n",
      "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Gradient Boosting Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, svm.SVC)\n",
      "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Support vector machine(SVM) LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, ensemble.RandomForestClassifier)\n",
      "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Random Forest Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, neighbors.KNeighborsClassifier)\n",
      "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, linear_model.LogisticRegression)\n",
      "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Logistic Regression LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = [0 for ii in y.tolist()]\n",
      "print('Dump Classifier:           {:.2f}'.format(metrics.accuracy_score(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred))); # ignore the warning as they are all 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scores seem very good, but if I predicted all of the labels 0, what would I get as accuracy rate? Let's see."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Dump Classifier: {:.2f}'.format(metrics.accuracy_score(y, [0 for ii in y.tolist()])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is because again unbalanced dataset problem. Since one class is 6 times of other class. I could get fairly accurate prediction if I predict the common class. What I need to look at as a metric if the classifier is doing well is to see the errors over the classes. Confusion matrices give a perfect way to see the distribution, which I will exactly use as a next step.\n",
      "\n",
      "Confusion Matrices\n",
      "If you have an unbalanced dataset problem or if you care accuracy of one class over other(maybe false-positives are not so bad for you but you definitely do not want any false negatives), then you could display the class accuracies in confusion matrices. Very common example of class preference of one to another is in medical applications. If you are trying to predict if a patient has cancer, you could get away with some false positives(patients that do not have cancer but are told to have cancers). However, you never ever want to tell a patient that has cancer that she does not have cancer(false negative). In this type of problems, not only you should be accurate but also you cannot be wrong in ne of the squares in the confusion matrix.\n",
      "\n",
      "Human problems cannot be solved by minimizing least squares error.\n",
      "\n",
      "Drew Conway\n",
      "\n",
      "Luckily, scikit learn provides a 2-D matrix for the confusion matrix under metrics submodule, which I will use to build confusion matrices for visualization(heatmap).\n",
      "\n",
      "If you are missing one tool in your machine learning workflow, search for if Scikit-Learn has an implementation. Chances are, it has.\n",
      "\n",
      "However, the heatmap in matplotlib implementation does not allow to print the matrix on top of the heatmap with default options. Heatmap is good in order to get a sense of where the classifier is not doing well, but not so good if you get numbers(you need to print out the confusion matrix separately). In order to combine both heatmap and also the numbers, I will use seaborn(an excellent statistical visualization library for Python) where it provides even advanced formatting for the numbers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pass_agg_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.PassiveAggressiveClassifier))\n",
      "grad_ens_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))\n",
      "decision_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, tree.DecisionTreeClassifier))\n",
      "ridge_clf_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.RidgeClassifier))\n",
      "svm_svc_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, svm.SVC))\n",
      "random_forest_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, ensemble.RandomForestClassifier))\n",
      "k_neighbors_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, neighbors.KNeighborsClassifier))\n",
      "logistic_reg_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.LogisticRegression))\n",
      "dumb_conf_matrix = metrics.confusion_matrix(y, [0 for ii in y.tolist()]); # ignore the warning as they are all 0\n",
      "\n",
      "conf_matrix = {\n",
      "                1: {\n",
      "                    'matrix': pass_agg_conf_matrix,\n",
      "                    'title': 'Passive Aggressive',\n",
      "                   },\n",
      "                2: {\n",
      "                    'matrix': grad_ens_conf_matrix,\n",
      "                    'title': 'Gradient Boosting',\n",
      "                   },\n",
      "                3: {\n",
      "                    'matrix': decision_conf_matrix,\n",
      "                    'title': 'Decision Tree',\n",
      "                   },\n",
      "                4: {\n",
      "                    'matrix': ridge_clf_conf_matrix,\n",
      "                    'title': 'Ridge',\n",
      "                   },\n",
      "                5: {\n",
      "                    'matrix': svm_svc_conf_matrix,\n",
      "                    'title': 'Support Vector Machine',\n",
      "                   },\n",
      "                6: {\n",
      "                    'matrix': random_forest_conf_matrix,\n",
      "                    'title': 'Random Forest',\n",
      "                   },\n",
      "                7: {\n",
      "                    'matrix': k_neighbors_conf_matrix,\n",
      "                    'title': 'K Nearest Neighbors',\n",
      "                   },\n",
      "                8: {\n",
      "                    'matrix': logistic_reg_conf_matrix,\n",
      "                    'title': 'Logistic Regression',\n",
      "                   },\n",
      "                9: {\n",
      "                    'matrix': dumb_conf_matrix,\n",
      "                    'title': 'Dumb',\n",
      "                   },\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fix, ax = plt.subplots(figsize=(16, 12))\n",
      "plt.suptitle('Confusion Matrix of Various Classifiers')\n",
      "for ii, values in conf_matrix.items():\n",
      "    matrix = values['matrix']\n",
      "    title = values['title']\n",
      "    plt.subplot(3, 3, ii) # starts from 1\n",
      "    plt.title(title);\n",
      "    sns.heatmap(matrix, annot=True,  fmt='');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seaborn is kind of neat, huh? Gradient Boosting is indeed quite good.\n",
      "\n",
      "Accuracy vs. Precision vs. Recall vs. F1 Score\n",
      "Accuracy, precision and recall are defined as follows respectively:\n",
      "accuracy=true positives + true negativestrue positives + true negatives + false positives + false negatives\n",
      "\n",
      "precision=true positivestrue positives + false positives\n",
      "\n",
      "recall=true positivestrue positives + false negatives\n",
      "\n",
      "Gradient Boosting Classifier not only is quite accurate for 0 class(not churning class) but also fairly accurate for churn class(class=1)).\n",
      "\n",
      "Generally, for unbalanced datasets, f1 score could also give a fairly accurate estimation of how well the classifier is doing considering both classes as it is the harmonic mean of precision and recall.\n",
      "f1=2\u22c5pr\u22c5rcpr+rc\n",
      "\n",
      "If we want to measure not just class distributions, but also a more abstract measures(like precision, recall or f1 score), we could get those measures by using classification_report function under the metrics submodule."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, linear_model.PassiveAggressiveClassifier)\n",
      "print('Passive Aggressive Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Passive Aggressive Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, ensemble.GradientBoostingClassifier)\n",
      "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Gradient Boosting Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, svm.SVC)\n",
      "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Support vector machine(SVM) LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, ensemble.RandomForestClassifier)\n",
      "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Random Forest Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, neighbors.KNeighborsClassifier)\n",
      "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, linear_model.LogisticRegression)\n",
      "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Logistic Regression LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = [0 for ii in y.tolist()]\n",
      "print('Dump Classifier:           {:.2f}'.format(metrics.accuracy_score(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred))); # ignore the warning as they are all 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-Learn provides a lot of metrics for your own evaluation. You could also build your own fbeta score if you want to weight precision more than recall or vice versa by using fbeta_score function under metrics in Scikit Learn.\n",
      "\n",
      "Search engines and generally information retrieval care more about precision than recall. A user could visit only so many webpages but when she visits first and second pages, she needs to see relevant/accurate results based on her query. Recall may not be very important for those cases as she is limited by time and recall may not be that important for the search results she sees."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gbc = ensemble.GradientBoostingClassifier()\n",
      "gbc.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get Feature Importance from the classifier\n",
      "feature_importance = gbc.feature_importances_\n",
      "# Normalize The Features\n",
      "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
      "sorted_idx = np.argsort(feature_importance)\n",
      "sorted_idx = sorted_idx[-280:]\n",
      "pos = np.arange(sorted_idx.shape[0]) + .5\n",
      "plt.figure(figsize=(16, 12))\n",
      "plt.barh(pos, feature_importance[sorted_idx], align='center', color='#7A68A6')\n",
      "plt.yticks(pos, np.asanyarray(df.columns.tolist())[sorted_idx])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.title('Variable Importance')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X2 = df[df.columns[sorted_idx[-280:]]].as_matrix().astype(np.float)\n",
      "shape(X2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scaled and Polynomial Features\n",
      "If you think, when some of the features come together, they could form a much more powerful feature, or just by getting the square of the feature would be powerful feature, then Scikit-Learn has something that quite fits to your needs. Let's aasume I have [x, y] feature vector and I am interested in [1, x, y, x^2, xy, y^2], in the preprocessing step, I could use PolynomialFeatures of Scikit-Learn to build that feature matrix. If I just want to only get the interaction features(not x^2, then it is enough to pass interaction_only=True and include_bias=False. If you want to get higher order Polynomial features(say nth degree), pass degree=n optional parameter to Polynomial Features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)\n",
      "polynomial_features = preprocessing.PolynomialFeatures()\n",
      "X = polynomial_features.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, linear_model.PassiveAggressiveClassifier)\n",
      "print('Passive Aggressive Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Passive Aggressive Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, ensemble.GradientBoostingClassifier)\n",
      "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Gradient Boosting Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, svm.SVC)\n",
      "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Support vector machine(SVM) LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, ensemble.RandomForestClassifier)\n",
      "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Random Forest Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, neighbors.KNeighborsClassifier)\n",
      "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, linear_model.LogisticRegression)\n",
      "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Logistic Regression LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = [0 for ii in y.tolist()]\n",
      "print('Dump Classifier:           {:.2f}'.format(metrics.accuracy_score(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred))); # ignore the warning as they are all 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)\n",
      "scaler = preprocessing.StandardScaler()\n",
      "X = scaler.fit_transform(X)\n",
      "polynomial_features = preprocessing.PolynomialFeatures()\n",
      "X = polynomial_features.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = stratified_cv(X, y, linear_model.PassiveAggressiveClassifier)\n",
      "print('Passive Aggressive Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Passive Aggressive Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, ensemble.GradientBoostingClassifier)\n",
      "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Gradient Boosting Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, svm.SVC)\n",
      "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Support vector machine(SVM) LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, ensemble.RandomForestClassifier)\n",
      "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Random Forest Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, neighbors.KNeighborsClassifier)\n",
      "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('K Nearest Neighbor Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = stratified_cv(X, y, linear_model.LogisticRegression)\n",
      "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, y_pred)))\n",
      "print('Logistic Regression LogLoss:\\n {}\\n'.format(llfun(y, y_pred)))\n",
      "y_pred = [0 for ii in y.tolist()]\n",
      "print('Dump Classifier:           {:.2f}'.format(metrics.accuracy_score(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, y_pred))); # ignore the warning as they are all 0\n",
      "print('Dump Classifier LogLoss:\\n {}\\n'.format(llfun(y, y_pred))); # ignore the warning as they are all 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}