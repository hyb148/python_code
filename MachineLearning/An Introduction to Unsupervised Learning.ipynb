{
 "metadata": {
  "name": "",
  "signature": "sha256:1ac1d17b133ca24909cb93e3e507299b86115e62a1b28eb465ef4b7f861f48d4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# http://bugra.github.io/work/notes/2014-11-16/an-introduction-to-unsupervised-learning-scikit-learn/\n",
      "\n",
      "Unsupervised Learning\n",
      "Unsupervised learning is the most applicable subfield on machine learning as it does not require any labels in the dataset and world is itself is an abundance of dataset. Human beings and their actions are recorded more and more every day(through photographs in Instagram, health data through wearables, internet activity through cookies and so on). Even the part of our lives which are not digital will be recorded in near future thanks to internet of things. In such a diversified and unlabeled dataset, unsupervised learning will become more and more important in the future.\n",
      "\n",
      "Not only it could be useful for dimensionality reduction in the feature set(like a preprocessing step) but also could be useful as feature extraction method. PCA(Principal Component Analysis) could be one of the most used unsupervised learning algorithm(PCA to unsupervised learning, linear regression equivalent to regression). It could be used both a dimensionality reduction in order to reduce data but also while it compresses(reduces) data, since it tries to capture the variance, it could pick up interesting featurse, so could be used as a feature extraction method.\n",
      "\n",
      "In this notebook, I will use PCA to both reduce dimensionality in the dataset and also build our feature vector. This specific method is called EigenFace(due to PCA extracting the eigenvectors and they could be visualized as face).\n",
      "\n",
      "Dimensionality Reduction & Feature Extraction via PCA (EigenFace)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import itertools\n",
      "import numpy as np\n",
      "import matplotlib as mlp\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import scipy\n",
      "\n",
      "\n",
      "from sklearn import cluster\n",
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "from sklearn.neighbors import kneighbors_graph\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "from sklearn import decomposition # PCA\n",
      "\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Modified Olivetti faces dataset\n",
      "faces = datasets.olivetti_faces.fetch_olivetti_faces()\n",
      "\n",
      "print(faces.DESCR)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "faces_images = faces['images']\n",
      "faces_data = faces.data\n",
      "\n",
      "faces_images.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Images are nothing but reshaped faces_data. It makes it easier for us to visualize the faces in an image grid but, we will use the faces_data to apply PCA. This is because PCA expects two dimensions (n_observations, n_dimensions)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "faces_data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have 400 instances and their vector dimension is 4096(256 * 256). We could reduce this dimension by applying PCA(Principal Component Analysis) and also PCA would extract features as well.\n",
      "\n",
      "Let's see some faces."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(16, 16))\n",
      "for ii in range(64):\n",
      "    plt.subplot(8, 8, ii + 1) # It starts with one\n",
      "    plt.imshow(faces_images[ii], cmap=plt.cm.gray)\n",
      "    plt.grid(False);\n",
      "    plt.xticks([]);\n",
      "    plt.yticks([]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try first number of eigenfaces 16 and then see what type of eigenfaces we get. Note that I am also passing the whiten=True in the PCA in order to remove the low-frequency(constant) areas in the face as those areas are not as important as areas that have variation and change(PCA also makes an assumption on the data very similarly)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_eigenfaces = 16\n",
      "# Creating PCA object\n",
      "pca = decomposition.RandomizedPCA(n_components=n_eigenfaces, whiten=True)\n",
      "# We are applying PCA to the data\n",
      "pca.fit(faces_data)\n",
      "\n",
      "pca.components_.shape\n",
      "\n",
      "plt.figure(figsize=(16, 16));\n",
      "plt.suptitle('EigenFaces');\n",
      "for ii in range(pca.components_.shape[0]):\n",
      "    plt.subplot(4, 4, ii + 1) # It starts with one\n",
      "    plt.imshow(pca.components_[ii].reshape(64, 64), cmap=plt.cm.gray)\n",
      "    plt.grid(False);\n",
      "    plt.xticks([]);\n",
      "    plt.yticks([]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with plt.style.context('fivethirtyeight'):\n",
      "    plt.figure(figsize=(16, 12));\n",
      "    plt.title('Explained Variance Ratio over Component');\n",
      "    plt.plot(pca.explained_variance_ratio_);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with plt.style.context('fivethirtyeight'):\n",
      "    plt.figure(figsize=(16, 12));\n",
      "    plt.title('Cumulative Explained Variance over EigenFace');\n",
      "    plt.plot(pca.explained_variance_ratio_.cumsum());"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('PCA captures {:.2f} percent of the variance in the dataset'.format(pca.explained_variance_ratio_.sum() * 100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is kind of low, we generally want 95% to be able to say, we could represent the dataset quite accurately with small number of dimensions. Let's increase the number of dimensions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_eigenfaces = 121\n",
      "# Creating PCA object\n",
      "pca = decomposition.RandomizedPCA(n_components=n_eigenfaces, whiten=True)\n",
      "# We are applying PCA to the data\n",
      "pca.fit(faces_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with plt.style.context('fivethirtyeight'):\n",
      "    plt.figure(figsize=(16, 12));\n",
      "    plt.title('Cumulative Explained Variance over EigenFace ');\n",
      "    plt.plot(pca.explained_variance_ratio_.cumsum());"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('PCA captures {:.2f} percent of the variance in the dataset'.format(pca.explained_variance_ratio_.sum() * 100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is good enough. Let's look what kind of eigenfaces we have this time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(16, 16));\n",
      "plt.suptitle('EigenFaces');\n",
      "for ii in range(pca.components_.shape[0]):\n",
      "    plt.subplot(11, 11, ii + 1) # It starts with one\n",
      "    plt.imshow(pca.components_[ii].reshape(64, 64), cmap=plt.cm.gray)\n",
      "    plt.grid(False);\n",
      "    plt.xticks([]);\n",
      "    plt.yticks([]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pointers\n",
      "  - PCA and Its applications | Application on Yale Face Databasehttp://bugra.github.io/work/notes/2013-07-27/PCA-EigenFace-And-All-That/\n",
      "  - EigenFace Library for Yale Face Database https://github.com/bugra/EigenFace\n",
      "  - Geometric Explanation of EigenFace http://bugra.github.io/work/notes/2014-09-27/geometric-take-on-pca/\n",
      "  - Original EigenFace Paper http://www.cs.ucsb.edu/~mturk/Papers/mturk-CVPR91.pdf\n",
      "\n",
      "Clustering Algorithms Comparison on Toy Datasets\n",
      "Let's generate some datasets. Put random seed=0 for reproducibility. The datasets are synthetic and I added a little noise on top of every dataset in order not to have very clean signals.\n",
      "\n",
      "It is always good practice to test your machine learning pipeline on the synthetic datasets similar to original dataset that you have due to two reasons; first you could measure the effectiveness relatively easy dataset and if the results are not promising you would not waste your time to prepare the original dataset, second it is easy to compare different clustering algorithms on well-known dataset whereas it might be harder to compare in an original dataset(e.g. if we do not know how many clusters that we have in dataset).\n",
      "\n",
      "Especially, if you do not have knowledge on how all the clustering methods work and want to simply experiment, the datasets that Scikit-learn provides becomes quite handy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(0)\n",
      "\n",
      "n_samples = 2500\n",
      "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
      "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
      "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
      "circles = datasets.make_circles(n_samples=n_samples, noise=.05)\n",
      "s_curve = datasets.make_s_curve(n_samples=n_samples, noise=.05)\n",
      "swiss_roll = datasets.make_swiss_roll(n_samples=n_samples, noise=.05)\n",
      "\n",
      "s_curve = np.vstack((s_curve[0][:, 0], s_curve[0][:, 2])).T, None\n",
      "swiss_roll = np.vstack((swiss_roll[0][:,0], swiss_roll[0][:,2])).T, None\n",
      "\n",
      "data_samples = [\n",
      "                noisy_circles, \n",
      "                noisy_moons, \n",
      "                blobs, \n",
      "                circles, \n",
      "                s_curve, \n",
      "                swiss_roll\n",
      "               ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Put six different clustering algorithms to compare their performances on the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "colors = np.array([ii.strip() for ii in '#30a2da, #fc4f30, #e5ae38, #6d904f, #8b8b8b'.split(',')])\n",
      "colors = np.hstack([colors] * 200)\n",
      "plt.figure(figsize=(20, 16))\n",
      "with plt.style.context('fivethirtyeight'):\n",
      "    plt.subplots_adjust(left=.001, right=.999, bottom=.001, top=.96, wspace=.05,\n",
      "                        hspace=.01)\n",
      "\n",
      "    plot_num = 1\n",
      "    for ii, dataset in enumerate(data_samples):\n",
      "        X, y = dataset\n",
      "        # normalize dataset for easier parameter selection\n",
      "        X = StandardScaler().fit_transform(X)\n",
      "\n",
      "        # estimate bandwidth for mean shift\n",
      "        bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)\n",
      "\n",
      "        # connectivity matrix for structured Ward\n",
      "        connectivity = kneighbors_graph(X, n_neighbors=10)\n",
      "        # make connectivity symmetric\n",
      "        connectivity = 0.5 * (connectivity + connectivity.T)\n",
      "\n",
      "        # Compute distances\n",
      "        distances = metrics.euclidean_distances(X)\n",
      "        if ii == 2:\n",
      "            n_clusters = 3\n",
      "        else:\n",
      "            n_clusters = 2\n",
      "        # create clustering estimators\n",
      "        ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
      "        two_means = cluster.MiniBatchKMeans(n_clusters=n_clusters)\n",
      "        ward = cluster.AgglomerativeClustering(n_clusters=n_clusters,\n",
      "                        linkage='ward', connectivity=connectivity)\n",
      "        spectral = cluster.SpectralClustering(n_clusters=n_clusters,\n",
      "                                              eigen_solver='arpack',\n",
      "                                              affinity=\"nearest_neighbors\")\n",
      "        dbscan = cluster.DBSCAN(eps=.2)\n",
      "        affinity_propagation = cluster.AffinityPropagation(damping=.9,\n",
      "                                                           preference=-200)\n",
      "\n",
      "        average_linkage = cluster.AgglomerativeClustering(linkage=\"average\",\n",
      "                                affinity=\"cityblock\", n_clusters=n_clusters,\n",
      "                                connectivity=connectivity)\n",
      "\n",
      "        \n",
      "\n",
      "        clustering_algos = [\n",
      "                    ('MiniBatchKMeans', two_means),\n",
      "                    ('AffinityPropagation', affinity_propagation),\n",
      "        #           ('MeanShift', ms),\n",
      "                    ('SpectralClustering', spectral),\n",
      "                    ('Ward', ward),\n",
      "                    ('AgglomerativeClustering', average_linkage),\n",
      "                    ('DBSCAN', dbscan)\n",
      "                   ]\n",
      "\n",
      "        for name, algorithm in clustering_algos:\n",
      "            # predict cluster memberships\n",
      "            t0 = time.time()\n",
      "            algorithm.fit(X)\n",
      "            t1 = time.time()\n",
      "            if hasattr(algorithm, 'labels_'):\n",
      "                y_pred = algorithm.labels_.astype(np.int)\n",
      "            else:\n",
      "                y_pred = algorithm.predict(X)\n",
      "\n",
      "            plt.subplot(len(clustering_algos), len(data_samples), plot_num)\n",
      "            if ii == 0:\n",
      "                plt.title(name, size=18)\n",
      "            plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
      "\n",
      "            if hasattr(algorithm, 'cluster_centers_'):\n",
      "                centers = algorithm.cluster_centers_\n",
      "                center_colors = colors[:len(centers)]\n",
      "                plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
      "            plt.xlim(-2, 2)\n",
      "            plt.ylim(-2, 2)\n",
      "            plt.xticks(())\n",
      "            plt.yticks(())\n",
      "            plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
      "                     transform=plt.gca().transAxes, size=15,\n",
      "                     horizontalalignment='right')\n",
      "            plot_num += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some Questions to Think About\n",
      "In the 4the dataset, since the inner circle is quite close to outer circle, all of the algorithms performed poorly. Can you think any trick that may be able to separate before applying algorithm? Does Squaring x and y values would resolve problem?\n",
      "Clustering is ill-posed problem because most of the time you may not know how many clusters you have in the dataset. Determining the number of clusters could be another part of the problem. Unsurprisingly, Scikit-Learn provides clustering evaluation metrics as well. metrics.mutual_info_score, metrics.silhouette_score are two popular ones.\n",
      "If you are doing text or document clustering, chances are that you want to do topic modeling as well. LDA(Latent Dirichlet Allocation) is not implemented in Scikit-Learn (at least yet)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}